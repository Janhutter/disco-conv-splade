{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82a03472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import gc\n",
    "import csv\n",
    "import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "\n",
    "prompt = \"\"\"# Instruction: I will give you a conversation between a user and a system. You should rewrite the last question of the user into a self-contained query. \n",
    "# Example 1: \n",
    "# Context: \n",
    "user: Tell me about the benefit of Yoga? \n",
    "system: Increased flexibility, muscle strength. \n",
    "# Please rewrite the following user question: \n",
    "Does it help in reducing stress? \n",
    "# Re-written query: Does Yoga help in reducing stress? \n",
    "# Example 2: \n",
    "# Context: \n",
    "{ctx}\n",
    "# Please rewrite the following user question: \n",
    "{question}\n",
    "# Re-written query:\n",
    "\"\"\"\n",
    "\n",
    "def _process_passage_chunk(rows):\n",
    "    passages_chunk = {}\n",
    "    for row in rows:\n",
    "        if not row:\n",
    "            continue\n",
    "        doc_id = row[0]\n",
    "        passage_text = \"\\t\".join(row[1:])\n",
    "        passages_chunk[doc_id] = passage_text\n",
    "    return passages_chunk\n",
    "\n",
    "def _load_passages_multiprocess(passage_loc, n_workers=None, chunk_size=1000, max_rows=10):\n",
    "    if n_workers is None:\n",
    "        n_workers = max(1, cpu_count() - 1)\n",
    "\n",
    "    passages = {}\n",
    "    with open(passage_loc, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        chunk = []\n",
    "        with Pool(processes=n_workers) as pool:\n",
    "            results = []\n",
    "            # tqdm progress bar over rows (streaming, capped by max_rows)\n",
    "            for i, row in enumerate(tqdm.tqdm(reader, desc=\"Loading passages\", unit=\"row\")):\n",
    "                if i >= max_rows:\n",
    "                    break\n",
    "                chunk.append(row)\n",
    "                if len(chunk) >= chunk_size:\n",
    "                    results.append(pool.apply_async(_process_passage_chunk, (chunk,)))\n",
    "                    chunk = []\n",
    "            if chunk:\n",
    "                results.append(pool.apply_async(_process_passage_chunk, (chunk,)))\n",
    "\n",
    "            for r in tqdm.tqdm(results, desc=\"Merging chunks\", unit=\"chunk\") :\n",
    "                passages.update(r.get())\n",
    "    return passages\n",
    "\n",
    "def generate_rewrites(questions_loc, qrels_loc, passage_loc, model_name, output_name):\n",
    "    # questions file has two columns: [\"0\" (row id), question text]\n",
    "    entries = pd.read_csv(questions_loc, sep='\\t', header=None, names=[\"row_id\", \"question\"])\n",
    "    qrels = pd.read_json(qrels_loc).to_dict(orient='index')\n",
    "\n",
    "    print('loading')\n",
    "    # load passages with multiprocessing to speed up large collections (only first 10k rows)\n",
    "    passages = _load_passages_multiprocess(passage_loc, max_rows=10000)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    # model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    # if tokenizer.pad_token is None:\n",
    "        # tokenizer.pad_token = tokenizer.eos_token\n",
    "    # gen_pipeline = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "    all_prompts = []\n",
    "    ids = []\n",
    "\n",
    "    rewrites = {}\n",
    "\n",
    "    # align each TSV row with qrels by position, not by key\n",
    "    qrels_items = list(qrels.items())\n",
    "\n",
    "    for row_idx, row in entries.iterrows():\n",
    "        question = row[\"question\"]\n",
    "        # get the qrels entry by position\n",
    "        _, qrel_for_row = qrels_items[row_idx]\n",
    "        # qrel_for_row looks like {\"5498209\": 1} or {5498209: 1}\n",
    "        doc_id = next(iter(qrel_for_row.keys()))\n",
    "        # robustly handle string/int ids\n",
    "        doc_id_str = str(doc_id)\n",
    "        if doc_id_str not in passages:\n",
    "            try:\n",
    "                doc_id_str = str(int(doc_id))\n",
    "            except Exception:\n",
    "                pass\n",
    "        passage = passages.get(doc_id_str, \"\")\n",
    "        print(\"QUESTION:\", question)\n",
    "        print(\"PASSAGE:\", passage)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e618e18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading\n"
     ]
    }
   ],
   "source": [
    "generate_rewrites('~/disco-conv-splade/DATA/topiocqa_subset/queries_rowid_train_all.tsv', '~/disco-conv-splade/DATA/topiocqa_subset/qrel_rowid_train.json', '/home/scur1719/disco-conv-splade/DATA/topiocqa_subset/full_wiki_segments_topiocqa.tsv', 'google-t5/t5-base', '~/disco-conv-splade/DATA/topiocqa_topiocqa/topiocqa_t5_rewrites.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a186090c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
